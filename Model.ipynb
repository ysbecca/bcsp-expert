{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network model class \n",
    "\n",
    "Can be adjusted for future use.\n",
    "Author @ysbecca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case start, end:  0 50\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import scripts.cnn_helper as cn\n",
    "import scripts.dataset as ds\n",
    "from tensorflow.python.client import device_lib\n",
    "import math\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scripts.dataset' from '/Users/ysbecca/ysbecca-projects/bcsp-expert/scripts/dataset.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(cn)\n",
    "reload(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have the model architecture parameters defined in a config file called `model_config.py`. For this notebook, the contents of `model_config` will be as defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case start, end:  0 2\n"
     ]
    }
   ],
   "source": [
    "# Input parameters\n",
    "img_size = 256\n",
    "num_channels = 9\n",
    "\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Convolutional layer parameters\n",
    "filter_sizes = [7, 5, 3, 1]\n",
    "num_filters = [16, 16, 16, 32]\n",
    "num_layers = len(filter_sizes)\n",
    "max_pools = [2, 2, 2, 2]\n",
    "relu = [1, 1, 1, 1]\n",
    "\n",
    "fc_sizes = [256, 128]\n",
    "num_classes = 2\n",
    "\n",
    "# Training params\n",
    "train_batch_size = 4\n",
    "test_batch_size = 4\n",
    "\n",
    "\n",
    "# Directories\n",
    "model_dir = \"\"\n",
    "checkpoints_dir = \"\"\n",
    "\n",
    "\n",
    "# To read the case numbers selected\n",
    "csv_name = \"/Users/ysbecca/ysbecca-projects/bcsp-expert/data/temp_cases.csv\"\n",
    "\n",
    "# General high directory containing all the cases\n",
    "img_dir = \"/Users/ysbecca/ysbecca-projects/bcsp-expert/data/wsi_samples/\"\n",
    "\n",
    "# Where to save the created h5 and csv files\n",
    "test_db_dir = \"/Users/ysbecca/ysbecca-projects/bcsp-expert/data/temp_db/\"\n",
    "\n",
    "# Where to find the JSON or XML annotation files\n",
    "xml_dir = \"/Users/ysbecca/ysbecca-projects/bcsp-expert/annotations/test_set/\"\n",
    "\n",
    "\n",
    "# For patch sampling =========================================================\n",
    "case_start = 0\n",
    "case_end = 2\n",
    "\n",
    "print(\"Case start, end: \", case_start, case_end)\n",
    "\n",
    "folder_prefix = \"Case_\"\n",
    "base_patch_size = 256\n",
    "patch_sizes = [512, 1024, 2048]\n",
    "level = 0\n",
    "pixel_overlap = 0\n",
    "samples_per_patch = len(patch_sizes)\n",
    "\n",
    "\n",
    "chunk_size = 2000\n",
    "\n",
    "# For image quality downsampling.\n",
    "downsamples = [2, 4, 8]\n",
    "\n",
    "# Downsampling annotated regions for quickly labeling patches\n",
    "annotation_downsample = 10\n",
    "\n",
    "label_map = {\"EP\": 0,\n",
    "             \"SM\": 1,\n",
    "             \"U\": -1} # Unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model():\n",
    "    \n",
    "    def __init__(self, \n",
    "                 total_k,\n",
    "                 name=\"CNN Model\",\n",
    "                 learning_rate=1e-4,\n",
    "                 pretrained_epochs=0,\n",
    "                 pretrained_model=False,\n",
    "                 weights_array=[],\n",
    "                ):\n",
    "        ''' Initalises all the network parameters and starts a TF session. '''\n",
    "        \n",
    "        self.total_k = total_k\n",
    "        self.weights_array = weights_array\n",
    "        self.epochs = pretrained_epochs\n",
    "        \n",
    "        # Load pre-trained model, if provided.\n",
    "        self.pretrained_model = pretrained_model\n",
    "    \n",
    "        self.name = name\n",
    "    \n",
    "        self.valid = None\n",
    "        self.train = None\n",
    "        \n",
    "        # Initialises the TF session and loads pretrained model if it exists.\n",
    "        \n",
    "        # Build tensors.\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\"x\")\n",
    "        self.x_image = tf.reshape(self.x, [-1, img_size, img_size, num_channels], name=\"x_image\")\n",
    "        \n",
    "        #self.weights = tf.placeholder(tf.float32, shape=[None], name='weights')\n",
    "        \n",
    "        self.y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name=\"y_true\")\n",
    "        self.y_true_cls = tf.argmax(self.y_true, dimension=1, name=\"y_true_cls\")\n",
    "\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\") # Dropout control.\n",
    "        self.gpus = self.__get_gpus()\n",
    "        \n",
    "        \n",
    "        if len(self.gpus) > 0:\n",
    "            # Remember that this adds significant latency for CPU<->GPU copying of shared variables.\n",
    "            # 2 GPU's is enough to get a good balance between speedup and minimal latency (12 GB on k80 nodes)\n",
    "            self.y_pred, self.y_pred_cls, self.cost = self.make_parallel(self.model, \n",
    "                                                     x_image_=self.x_image, \n",
    "                                                     y_true_=self.y_true)\n",
    "                                                     #weights_=self.weights)\n",
    "            total_batches = len(self.gpus)\n",
    "        else:\n",
    "            # CPU-only version\n",
    "            self.y_pred, self.y_pred_cls, self.cost = self.model(x_image_=self.x_image, \n",
    "                                                       y_true_=self.y_true)\n",
    "                                                       #weights_=self.weights)\n",
    "            total_batches = 1\n",
    "\n",
    "            \n",
    "        self.train_batch_size = train_batch_size * total_batches\n",
    "        self.test_batch_size = test_batch_size * total_batches\n",
    "    \n",
    "        self.optimizer = tf.train.AdagradOptimizer(learning_rate=\n",
    "                                              learning_rate).minimize(self.cost, \n",
    "                                                                      colocate_gradients_with_ops=True)\n",
    "        correct_prediction = tf.equal(self.y_pred_cls, self.y_true_cls)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "        if len(self.gpus) > 0: # Log GPU/CPU placement to the terminal if using GPU's.\n",
    "            self.session = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "        else:\n",
    "            self.session = tf.Session()\n",
    "\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        # Load pretrained model\n",
    "        if self.pretrained_model:\n",
    "            self.restore_model(self.pretrained_model)\n",
    "    \n",
    "    \n",
    "    def model(self, x_image_, y_true_):\n",
    "        ''' Defines the model which can be copied to each GPU device being used. '''\n",
    "\n",
    "        model = x_image_\n",
    "        num_filters_ = [num_channels] + num_filters\n",
    "        print(\"Num Filters:\", num_filters_)\n",
    "        print(\"Filter sizes:\", filter_sizes)\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            print(\"i:\", i)\n",
    "            print(\"Input channels:\", num_filters_[i])\n",
    "            print(\"Filter sizes:\", filter_sizes[i])\n",
    "            \n",
    "            model = cn.new_conv_layer(input=model,\n",
    "                                          num_input_channels=num_filters_[i],\n",
    "                                          filter_size=filter_sizes[i],\n",
    "                                          num_filters=num_filters_[i+1],\n",
    "                                          use_pooling=True,\n",
    "                                          max_pool_size=max_pools[i],\n",
    "                                          use_relu=relu[i])\n",
    "            model = tf.nn.dropout(model, self.keep_prob)\n",
    "            print(model)\n",
    "\n",
    "\n",
    "        model, num_fc_features = cn.flatten_layer(model)\n",
    "        print(num_fc_features)\n",
    "\n",
    "        if len(fc_sizes) > 0:\n",
    "            fc_sizes_ = [num_fc_features] + fc_sizes\n",
    "\n",
    "            # The flattened features will then be fed into the fully-connected layers.\n",
    "            for i in range(len(fc_sizes)):\n",
    "                model = cn.new_fc_layer(input=model,          \n",
    "                                     num_inputs=fc_sizes_[i],\n",
    "                                     num_outputs=fc_sizes_[i+1],\n",
    "                                     use_relu=True)\n",
    "                model = tf.nn.dropout(model, self.keep_prob)\n",
    "\n",
    "            num_fc_features = fc_sizes_[-1]\n",
    "\n",
    "        model = cn.new_fc_layer(input=model,          \n",
    "                                 num_inputs=num_fc_features,\n",
    "                                 num_outputs=num_classes,\n",
    "                                 use_relu=True)\n",
    "\n",
    "        y_pred = tf.nn.softmax(model)\n",
    "        y_pred_cls = tf.argmax(model, dimension=1)\n",
    "        cost = tf.reduce_mean(tf.losses.softmax_cross_entropy(logits=model, onehot_labels=y_true_))\n",
    "                                                              #, weights=weights_))\n",
    "        return y_pred, y_pred_cls, cost\n",
    "        \n",
    "    \n",
    "    def train(self, k, valid_k, epochs, verbose=True, dropout_keep_prob=0.9, show_valid_acc=True):\n",
    "        ''' Train the network for the given number of epochs. '''\n",
    "        print(\"in\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load k training set and valid_k set.\n",
    "        if not self.train:\n",
    "            self.load_train_dataset(k)\n",
    "        \n",
    "        if show_valid_acc and not self.valid:\n",
    "            self.load_valid_dataset(valid_k)\n",
    "        \n",
    "        # Calculate the number of iterations required for one epoch.\n",
    "        iterations_per_epoch = int(math.ceil(self.train.num_images / self.train_batch_size))\n",
    "        iterations = iterations_per_epoch * epochs\n",
    "        \n",
    "        # Optimise.\n",
    "        acc = self.optimize(self.train, iterations, dropout_keep_prob, verbose, show_valid_acc)\n",
    "        print(\"Epoch\", epochs, \"..................... Train accuracy:\", acc)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        time_dif = end_time - start_time\n",
    "        if verbose:\n",
    "            print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "    \n",
    "    def load_valid_dataset(self, valid_k):\n",
    "        self.valid = ds.read_k_dataset(valid_k, self.total_k, shuffle_all=True, do_augments=False)\n",
    "        \n",
    "    def load_train_dataset(self, train_k):\n",
    "        self.train = ds.read_k_dataset(train_k, self.total_k, shuffle_all=True, do_augments=True)\n",
    "    \n",
    "    def get_accuracy(self, dataset, show_conf_matrix=True, verbose=True, get_outputs=False):\n",
    "        ''' Computes validation/test accuracy; used during training and as a standalone function. Batch\n",
    "            size is test_batch_size since forward pass only. '''\n",
    "        \n",
    "        if not dataset:\n",
    "            print(\"OUPS! No dataset loaded.\")\n",
    "            return -1.0\n",
    "            \n",
    "        # Calculate accuracy.\n",
    "        num_test = len(dataset.images)\n",
    "\n",
    "        cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "        i = 0\n",
    "\n",
    "        while i < num_test:\n",
    "            j = min(i + test_batch_size, num_test)\n",
    "            curr_batch_size = j - i\n",
    "\n",
    "            # Get the images and targets from the test-set between index i and j.\n",
    "            images = dataset.images[i:j, :].reshape(curr_batch_size, img_size_flat)\n",
    "            labels = dataset.labels[i:j, :]\n",
    "            \n",
    "            print(images.dtype)\n",
    "            print(np.shape(images))\n",
    "            print(np.shape(labels))\n",
    "            \n",
    "            #weights_ = np.ones((curr_batch_size))\n",
    "            feed_dict = {self.x: images, self.y_true: labels, self.keep_prob: 1.0}\n",
    "            \n",
    "            if get_outputs: # NON thresholded outputs\n",
    "                cls_pred[i:j] = self.session.run(self.y_true, feed_dict=feed_dict)\n",
    "            else:\n",
    "                cls_pred[i:j] = self.session.run(self.y_pred_cls, feed_dict=feed_dict)\n",
    "            i = j\n",
    "\n",
    "        # TODO Define differently depending on valid / test and problem...\n",
    "        cls_true = [np.argmax(label) for label in dataset.labels]\n",
    "\n",
    "        # Create a boolean array whether each image is correctly classified.\n",
    "        correct = (cls_true == cls_pred)\n",
    "\n",
    "        correct_sum = correct.sum() # sum(1 for a, b in zip(cls_true, cls_pred) if a and b)\n",
    "        acc = float(correct_sum) / num_test\n",
    "\n",
    "        if verbose:\n",
    "            msg = \"ACCURACY: {0:.1%} ({1} / {2})\"\n",
    "            print(msg.format(acc, correct_sum, num_test))\n",
    "        \n",
    "        if show_conf_matrix:\n",
    "            cm = cn.plot_confusion_matrix(cls_true, cls_pred=cls_pred, show_plt=True)\n",
    "            \n",
    "        return acc, cm, cls_pred\n",
    "        \n",
    "    \n",
    "    def get_outputs(self, k, shuffle=False, augment=False, thresholded=True):\n",
    "        ''' Retrieves the softmax outputs for a random k set and returns. '''\n",
    "        dataset = self.load_k_set(k, shuffle=shuffle, augment=augment)\n",
    "        \n",
    "        acc, cm, cls_pred = self.get_accuracy(dataset, show_conf_matrix=False, verbose=False, get_outputs=thresholded)\n",
    "        \n",
    "        return cls_pred\n",
    "    \n",
    "    def save_outputs(self, k, outputs):\n",
    "        ''' Saves softmax outputs to a file. '''\n",
    "        \n",
    "        #TOOD save outputs into specified directory in a good format.\n",
    "        print(\"Oups! Not implemented yet.\")\n",
    "        return -1\n",
    "    \n",
    "    def load_k_set(self, k, shuffle, augment):\n",
    "        ''' Loads the kth set without setting the self.train or self.valid variables. '''\n",
    "        return ds.read_k_dataset(k, self.total_k, shuffle_all=shuffle, do_augments=augment)\n",
    "    \n",
    "    def __get_gpus(self):\n",
    "        return [x.name for x in device_lib.list_local_devices() if x.device_type == 'GPU']\n",
    "    \n",
    "    def make_parallel(self, fn, **kwargs):\n",
    "        ''' Splits the model across available GPUs. '''\n",
    "        in_splits = {}\n",
    "        for k, v in kwargs.items():\n",
    "            in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "        y_pred_split, y_pred_cls_split, cost_split = [], [], []\n",
    "        for i in range(num_gpus):\n",
    "            with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "                with tf.variable_scope(tf.get_variable_scope(), reuse=i > 0):\n",
    "                    y_pred_, y_pred_cls_, cost_ = fn(**{k : v[i] for k, v in in_splits.items()})\n",
    "                    y_pred_split.append(y_pred_)\n",
    "                    y_pred_cls_split.append(y_pred_cls_)\n",
    "                    cost_split.append(cost_)\n",
    "\n",
    "        return tf.concat(y_pred_split, axis=0), tf.concat(y_pred_cls_split, axis=0), \\\n",
    "                tf.stack(cost_split, axis=0)\n",
    "    \n",
    "    # Saving and restoring models.\n",
    "    def save_model(self, k, epochs=0):\n",
    "        model_name = 'm-' + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\") + \"-\" + str(k) + \"-\" + str(epochs)\n",
    "        self.saver.save(self.session, checkpoints_dir + model_name)\n",
    "        print(\"CNN Model saved for k=\", k, \":\", model_name)\n",
    "        \n",
    "        return model_name\n",
    "\n",
    "    def restore_model(self, model_name):\n",
    "        output = self.saver.restore(sess=self.session, save_path=checkpoints_dir + model_name)\n",
    "        print(output)\n",
    "\n",
    "    def optimize(self, dataset, num_iterations, dropout_keep_prob, verbose, show_valid_acc, valid_interval=100):\n",
    "        ''' Optimises for the given number of iterations by batches. '''\n",
    "        total_iterations = 0\n",
    "        \n",
    "        for i in range(total_iterations,\n",
    "                       total_iterations + num_iterations):\n",
    "            x_batch, y_true_batch = dataset.next_batch(train_batch_size)\n",
    "            x_batch = x_batch.reshape(len(x_batch), img_size_flat)\n",
    "\n",
    "            #weights_ = generate_batch_weights(y_true_pseudo)\n",
    "            feed_dict_train = {self.x: x_batch, self.y_true: y_true_batch, self.keep_prob: dropout_keep_prob}\n",
    "            \n",
    "            if i % valid_interval and show_valid_acc:\n",
    "                acc, _, _ = self.get_accuracy(self.valid, show_conf_matrix=False)\n",
    "                msg = \"Validation Accuracy:    {1:>6.1%}\"\n",
    "                print(msg.format(acc), \" Iterations: \", total_iterations)\n",
    "\n",
    "            self.session.run(self.optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        acc = self.session.run(self.accuracy, feed_dict=feed_dict_train)\n",
    "        total_iterations += num_iterations\n",
    "        \n",
    "        if verbose:\n",
    "            msg = \"Training Accuracy:      {1:>6.1%}\"\n",
    "            print(msg.format(acc), \" Iterations: \", total_iterations)\n",
    "        \n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Filters: [9, 16, 16, 16, 32]\n",
      "Filter sizes: [7, 5, 3, 1]\n",
      "i: 0\n",
      "Input channels: 9\n",
      "Filter sizes: 7\n",
      "Tensor(\"Conv2D_8:0\", shape=(?, 256, 256, 16), dtype=float32)\n",
      "Tensor(\"Relu_14:0\", shape=(?, 256, 256, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_8:0\", shape=(?, 128, 128, 16), dtype=float32)\n",
      "Tensor(\"dropout_12/mul:0\", shape=(?, 128, 128, 16), dtype=float32)\n",
      "i: 1\n",
      "Input channels: 16\n",
      "Filter sizes: 5\n",
      "Tensor(\"Conv2D_9:0\", shape=(?, 128, 128, 16), dtype=float32)\n",
      "Tensor(\"Relu_15:0\", shape=(?, 128, 128, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_9:0\", shape=(?, 64, 64, 16), dtype=float32)\n",
      "Tensor(\"dropout_13/mul:0\", shape=(?, 64, 64, 16), dtype=float32)\n",
      "i: 2\n",
      "Input channels: 16\n",
      "Filter sizes: 3\n",
      "Tensor(\"Conv2D_10:0\", shape=(?, 64, 64, 16), dtype=float32)\n",
      "Tensor(\"Relu_16:0\", shape=(?, 64, 64, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_10:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "Tensor(\"dropout_14/mul:0\", shape=(?, 32, 32, 16), dtype=float32)\n",
      "i: 3\n",
      "Input channels: 16\n",
      "Filter sizes: 1\n",
      "Tensor(\"Conv2D_11:0\", shape=(?, 32, 32, 32), dtype=float32)\n",
      "Tensor(\"Relu_17:0\", shape=(?, 32, 32, 32), dtype=float32)\n",
      "Tensor(\"MaxPool_11:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "Tensor(\"dropout_15/mul:0\", shape=(?, 16, 16, 32), dtype=float32)\n",
      "8192\n"
     ]
    }
   ],
   "source": [
    "cnn_model = CNN_Model(total_k=10, name=\"Test-CNN-Model\", pretrained_epochs=0, pretrained_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CNN_Model object at 0x128072ba8>\n",
      "Test-CNN-Model\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(cnn_model)\n",
    "print(cnn_model.name)\n",
    "print(cnn_model.total_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cases:            ['0025', '0037']\n",
      "Ground truth:     []\n",
      "Selected:         [ 1.  0.]\n",
      "Before stacking:  (3, 12, 256, 256, 3)\n",
      "After stacking:   (12, 256, 256, 9)\n",
      "LOADED k-set:     0\n",
      "(12, 256, 256, 9)\n",
      "(12, 2)\n",
      "(12, 2)\n",
      "(12,)\n"
     ]
    }
   ],
   "source": [
    "dataset = cnn_model.load_k_set(0, shuffle=False, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "(4, 589824)\n",
      "(4, 2)\n",
      "float32\n",
      "(4, 589824)\n",
      "(4, 2)\n",
      "float32\n",
      "(4, 589824)\n",
      "(4, 2)\n",
      "ACCURACY: 100.0% (12 / 12)\n",
      "[[12]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARAAAAD0CAYAAABTqHk1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEqRJREFUeJzt3X+sXnVhx/H3pwWVQCOSFmKhBKKMSaoU01SdZhMcrDYo\nw80EXHRMkkY3F4hs/gjL1C3L3FhM5mDZqiXMjaFM7UKkUspCVrvwq2CpQEGBaSiQlcpQQAf03s/+\nOKfyUO69z3nOfc5zznOfzys54T7nnvN8vw+393O/v845sk1ERB2L2q5ARIyvBEhE1JYAiYjaEiAR\nUVsCJCJqS4BERG0JkIioLQESEbUlQCKitkParkDEJPuN0w/3j5+cqnTsnbue22J7bcNVGkgCJKJF\n+56c4rYtx1U69tDXPrS04eoMLAES0Soz5em2K1FbAiSiRQamGd8LWhMgES2bJi2QiKjBmKkxvqXG\nRE3jSlor6QFJD0r6VNv1qUvSlZL2Srqn7brMl6QVkm6WdJ+keyVd1Had6pL0Kkm3S7q7/Cyfq3Le\nNK60ddHEBIikxcAVwLuBU4DzJZ3Sbq1quwro1HTePOwHLrF9CvBW4A/G+OfyHHCG7VOBVcBaSW+d\n6wQDU7jS1kWT1IVZAzxo+2EASV8FzgHua7VWNdjeJumEtusxDLYfBx4vv35a0m7gWMbz52LgmfLl\noeU252++gRfGeBZmYlogFP8oH+l5vafcFx1RhuJpwG3t1qQ+SYsl7QT2Altt9/0s0xW3LpqkAIkO\nk3QE8A3gYts/bbs+ddmesr0KOA5YI2nlnMdX7L50tQszSQHyKLCi5/Vx5b5omaRDKcLjatvfbLs+\nw2D7KeBm+o1VGaYqbl00SQFyB3CSpBMlvQI4D7iu5TpNPEkCNgK7bX+h7frMh6Rlko4svz4MOBO4\nf65zioVk6cJ0nu39wMeALcBu4Frb97Zbq3okXQPcApwsaY+kC9uu0zy8HfggcIakneW2ru1K1fRa\n4GZJuyj+YG21/a25TxFTFbcumqRZGGxvBja3XY/5sn1+23UYFtvboaO/HQOyvYtiELj6OcB0R7sn\nVUxMCySiiww8z6JKWxUzLTKU9OeSdpWtuxslLZ/l3IEXWiZAIlo2bVXaKrqKlw/cXmb7TeXs0LeA\nPz34pLoLLRMgES0qVqIObwzE9jbgyYP29U6LH87Mi9t+sdDS9vPAgYWWc5qoMZCIrjFiagR/xyX9\nBfAh4CfA6TMcMtNCy7f0e9+Ja4FIWt92HYYln6WbBv0sA3Rhlkra0bNVLsf2pbZXAFdTzEYOxcQF\nCLBg/qGSz9JV1X+xGagLs8/26p5tQ426XQ381gz7ay20nMQAiegQMeVFlbbaJUgn9bw8h5kXt9Va\naNmpMZDFRxzuQ446qtkyXvMaXnn8ipHMvL/x6GMaff/jjz+e1atXj/EqghctpM/Ci1fk9mXgBRYP\nreBykeE7Kbo7e4DPAOsknUyxoPVHwEfKY5cDX7a9zvZ+SQcWWi4Grqyy0LJTAXLIUUex/JKL267G\n0Oy46JK2qxAtkPRA1WNtzat18fL3m3GR4cZZjn0MWNfzeuCFlp0KkIhJND3GC3ETIBEtKgZRx3co\nMgES0arhdmFGLQES0aLicv4ESETUYMTzHt4szKglQCJaNp0uTETUkUHUiKjNiKnql+p3TgIkomUZ\nRI2IWmwyjRsRdSkrUSOiHgPPe3x/Dce35hELgBnofqedkwCJaFmmcSOiluK5MAmQiKilu0+dqyIB\nEtGitEAiYl7SAomIWmzxwvT4/hqOb80jFoDifiBpgURELeN9R7JGa17nad8Rk6QYRB3qw7VHqrEW\nSM/Tvs+keM7mHZKus31fU2VGjKNxXkjWZM1rPe07YpIcWMqeFsjLVXrad/mA4PVQPDUuYtLkfiDz\nUD4geAMwskdORnSFDS9MJ0BmUutp3xGTpOjCJEBm8ounfVMEx3nABxosL2IsZSXqDOo+7TtikhyY\nxh1XjY6B1Hnad8RkSRcmIuYhS9kjopbiruwJkIiowYj903k2bkTUlC5MRNSSWZiImJfMwkREPR2+\nUK6KBEhEi3JHsoiYl2G2QCRdCZwN7LW9stx3GfAe4HngIeD3bD81w7k/BJ4GpoD9tlf3K298O18R\nC4CB/dOLKm0VXQWsPWjfVmCl7TcB3wc+Pcf5p9teVSU8IAES0aph31DI9jbgyYP23Wh7f/nyVoor\n44ciARLRsmlUaQOWStrRs62vUdyHgW/P8j0DN0m6s+p7Zwwkok0eaAxkX9WuxUwkXQrsB66e5ZB3\n2H5U0tHAVkn3ly2aWaUFEtGiUd2VXdIFFIOrv2N7xjv/2X60/O9eYBPFfY3nlACJaFnTASJpLfAJ\n4L22fzbLMYdLWnLga+As4J5+750AiWiREVPTiyptVUi6BrgFOFnSHkkXApcDSyi6JTsl/UN57HJJ\nB+7XcwywXdLdwO3A9bZv6FdexkAiWjbMhWS2z59h98ZZjn0MWFd+/TBw6qDlJUAiWuTBBlE7JwES\n0TInQIbjjUcfw46LLmm7GhEjlIvpImIe0gKJiFpyQ6GIqC83VY6Iuky6MBFRWwZRI2IeZr4yZTwk\nQCJali5MRNRiJ0AiYh4yBhIRtU1PJ0AiogajdGEior4xnoRJgES0KoOoETEvY9wESYBEtCwtkIio\nLStRI6IWG1z9sZWdkwCJaFlaIBFRXwIkIurJQrKImI+0QCKiljFfSNbY8K+kKyXtldT3+ZoRE80V\ntw5qcv7oKmBtg+8fsTBY1bYOaixAbG8Dnmzq/SMWjLRA6pO0XtIOSTueeOKJtqsTMVomLZD5sL3B\n9mrbq5ctW9Z2dSJGrritYf+tizILE9G2joZDFZUDRNIrbT/XZGUiJlJHuydV9O3CSFoj6XvAD8rX\np0r6uwrnXQPcApwsaY+kC+dd24iFxqDpalsXVWmBfBE4G/h3ANt3Szq930m2z59n3SImQHcHSKuo\nEiCLbP9IesmHnGqoPhGTZ4GPgTwiaQ1gSYuBPwS+32y1IibIAg+Qj1J0Y44H/ge4qdwXEcMwxgHS\ndxDV9l7b59leWm7n2d43ispFLHhDXkg20zVoki6TdL+kXZI2STpylnPXSnpA0oOSPlWlvL4tEElf\nYoaMtL2+SgERMTcNtwVyFXA58JWefVuBT9veL+mvgE8Dn3xJHYrhiSuAM4E9wB2SrrN931yFVenC\n3NTz9auAc4FHKpwXEVUMMUBsb5N0wkH7bux5eSvw2zOcugZ40PbDAJK+CpwDzC9AbH+t97Wkfwa2\n9zsvIqoZcguknw8DX5th/7G8tGGwB3hLvzers5T9ROCYGudFxEyqrwNZKmlHz+sNtjdUPVnSpcB+\n4OoBajenKmMg/8uLjaxFFJfoVxpgiYg+BrtUf5/t1XWKkXQBxYLQd9kzXpr3KLCi5/Vx5b45zRkg\nKlaPndrzRtOzFB4RdTX8GyVpLfAJ4Nds/2yWw+4ATpJ0IsXv+3nAB/q995zTuGVYbLY9VW4Jj4gh\nk6ttld5r5mvQLgeWAFsl7ZT0D+WxyyVtBrC9H/gYsAXYDVxr+95+5VUZA9kp6TTb3632ESJiIMOd\nhZnpGrSNsxz7GLCu5/VmYPMg5c0aIJIOKVPpNIo54YeAZwEVZfnNgxQUES8nd/dK2yrmaoHcDrwZ\neO+I6hIxmRbo1bgCsP3QiOoSMZnGeGRxrgBZJunjs33T9hcaqE/ExBnxQrKhmitAFgNHULZEIqIh\nCzRAHrf9ZyOrScQkGmCKtov6joFERMMWaIC8a2S1iJhg4zyNO+tKVNt5LGVEzCkPlopo2wLtwkRE\n0xbwIGpEjEICJCJqS4BERB0iXZiIqGsBX40bEaOQFkhE1JYAiYi6MgYSEfUlQCKilsEe69A5CZCI\nlmUWJiJqyxhIRNSXAImIWjIGEhF1ifG+9V8CJKJtaYFERF0ZRI2I+jKNGxG15I5kETEvYxwgs96V\nfb4krZB0s6T7JN0r6aKmyooYZ3K1rYuabIHsBy6xfZekJcCdkrbavq/BMiPGT0fDoYrGAsT248Dj\n5ddPS9oNHAskQCJ6dLV1UUVjXZhekk4ATgNum+F76yXtkLTjiSeeGEV1IrrDA2wd1HiASDoC+AZw\nse2fHvx92xtsr7a9etmyZU1XJ6JTRHE1bpWtixqdhZF0KEV4XG37m02WFTG2Otq6qKKxAJEkYCOw\n2/YXmionYtzJ45sgTXZh3g58EDhD0s5yW9dgeRHjZ8zHQJqchdnOeF9oGDESw5yFkXQlcDaw1/bK\nct/7gc8CbwDW2N4xy7k/BJ4GpoD9tlf3K28kszARMYfhtkCuAtYetO8e4H3Atgrnn257VZXwgCxl\nj2jdMFsgtreVyyZ69+0GKIYlhystkIg2eaBp3KUH1kyV2/rh14abJN1Z9b3TAoloW/UWyL6qXYua\n3mH7UUlHA1sl3W97zm5PWiARLRLduZjO9qPlf/cCm4A1/c5JgES0za62NUjS4eVFr0g6HDiLYvB1\nTgmQiJYNswUi6RrgFuBkSXskXSjpXEl7gLcB10vaUh67XNLm8tRjgO2S7gZuB663fUO/8jIGEtGm\nIS8Ss33+LN/aNMOxjwHryq8fBk4dtLwESETLunqhXBUJkIiWJUAioh7T+ABpkxIgES0b5zuSJUAi\n2pYAiYg6DiwkG1cJkIg2jWCRWJMSIBEtyyxMRNSWLkxE1GNgenwTJAES0bbxzY8ESETb0oWJiPoy\nCxMRdaUFEhG1yKAMokZEbVkHEhF1jfOjLRMgEW3q8GMrq0iARLQq18JExDxkFiYi6ksLJCJqMWgq\nARIRdY1vfiRAItqWadyIqC8BEhG1mKxEjYh6hNOFiYh5SIBERC0GMo0bEXWlCxMR9SVAIqKeXEwX\nEXWZBEhEzEPWgbycpFcB24BXluV83fZnmiovYlxlEHVmzwFn2H5G0qHAdknftn1rg2VGjBcDU+Pb\nBGksQGwbeKZ8eWi5jW/URjQig6izkrQYuBN4PXCF7dtmOGY9sL58+YykB5qsE7AU2NdwGaOSz9JN\nJw909BADRNKVwNnAXtsry33vBz4LvAFYY3vHLOeuBf4WWAx82fbn+5XXaIDYngJWSToS2CRppe17\nDjpmA7ChyXr0krTD9upRldekfJZukjTjL+ishtsCuQq4HPhKz757gPcB/zjbSeUf+yuAM4E9wB2S\nrrN931yFLZpvbauw/RRwM7B2FOVFjA0D0662VXk7exvw5EH7dtvu17JfAzxo+2HbzwNfBc7pV15j\nASJpWdnyQNJhFMl2f1PlRYwng6erbc06Fnik5/Wect+cmuzCvBb4p7JptAi41va3GiyvqpF1l0Yg\nn6Wbqn+WwWZhlh7UPdpQDgG0pslZmF3AaU29f11t/w+vQtIU8D2Kn89u4Hdt/+zg46p8FknvBP7I\n9tmS3gucMtvgWNli/IDtvx+wvp8FnrH9N4Oc12scfi5VDfxZqo+B7GtwnOhRYEXP6+PKfXMayRhI\nDOzntleVo+jPAx/p/aYKA//sbF/XZ2T9SOD3B33fmCe72tasO4CTJJ0o6RXAecB1/U5KgHTfd4DX\nSzpB0gOSvkIxqr5C0lmSbpF0l6R/k3QEFNNxku6XdBfF6Dvl/gskXV5+fYykTZLuLrdfAT4PvE7S\nTkmXlcf9saQ7JO2S9Lme97pU0vclbWfQacvoUTE8KgaIpGuAW4CTJe2RdKGkcyXtAd4GXC9pS3ns\nckmbAWzvBz4GbKFo9V5r+95+5eVamA6TdAjwbuCGctdJFN2ZWyUtBf4E+HXbz0r6JPBxSX8NfAk4\nA3gQ+Nosb/9F4D9tn1uOUx0BfApYaXtVWf5ZZZlrAAHXSfpV4FmKv1CrKP4N3UWx3icGZWB6eAOk\nts+f5VubZjj2MWBdz+vNwOZBykuAdNNhknaWX38H2AgsB37UcynAW4FTgP+SBPAKir88vwz8t+0f\nAEj6F15cqNfrDOBD8Iv1Oj+R9JqDjjmr3L5bvj6CIlCWAJsOjMtI6tvUjTlkJWoM2c8PtAIOKEPi\n2d5dwNaD/+JIesl58yTgL22/ZAGSpIuHWEaMcYBkDGR83Qq8XdLrASQdLumXKNbanCDpdeVxszVp\n/wP4aHnuYkmvBp6maF0csAX4cM/YyrGSjqa4yvo3JR0maQnwniF/tslh46mpSlsXJUDGlO0ngAuA\nayTtouy+2P4/ii7L9eUg6t5Z3uIi4HRJ36MYvzjF9o8pukT3SLrM9o3AvwK3lMd9HVhi+y6KsZW7\ngW9TjOBHXUNciTpq8hg3nyLG3asPWea3Lem7YhyALU9tvLNr1wtlDCSiTfZQZ2FGLQES0bYx7gUk\nQCJa5rRAIqKe3JEsIuoy0NEp2ioSIBEtMuCOTtFWkQCJaJM9ipsFNSYBEtGycW6BZCFZRIsk3UBx\nR/oq9tnu1H2FEyARUVuuhYmI2hIgEVFbAiQiakuARERtCZCIqC0BEhG1JUAiorYESETUlgCJiNr+\nH7nmxXvSjh8NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x129c4fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1.0, None, array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.get_accuracy(dataset, show_conf_matrix=True, verbose=True, get_outputs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
