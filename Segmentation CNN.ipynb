{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation CNN\n",
    "\n",
    "@ysbecca\n",
    "Basic CNN to perform segmentation of training set based on layers:\n",
    "- epithelial layer\n",
    "- submucosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import scripts.cnn_helper as cn\n",
    "import scripts.dataset as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoints_dir = \"/Users/ysbecca/ysbecca-projects/bcsp-expert/checkpoints/\"\n",
    "\n",
    "# Image params\n",
    "img_size = 16\n",
    "num_channels = 3\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Convolutional layer params\n",
    "filter_sizes = [3, 3]\n",
    "num_filters = [16, 16]\n",
    "num_layers = len(filter_sizes)\n",
    "\n",
    "max_pools = [2, 2]\n",
    "\n",
    "# Fully connected layers, followed by classification layer.\n",
    "fc_1_size = 128\n",
    "fc_2_size = 64\n",
    "\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build TensorFlow graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')\n",
    "x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # So that we can control dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find device configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU devices found: []\n"
     ]
    }
   ],
   "source": [
    "gpus = [x.name for x in device_lib.list_local_devices() if x.device_type == 'GPU']\n",
    "num_gpus = len(gpus)\n",
    "print(\"GPU devices found: \" + str(gpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(x_image_, y_true_):\n",
    "    ''' Expecting the following parameters, in batches:\n",
    "        x_image_ - x_image\n",
    "        y_true_ - y_true\n",
    "    '''\n",
    "    network, _ = cn.new_conv_layer(input=x_image_,\n",
    "                                  num_input_channels=num_channels,\n",
    "                                  filter_size=filter_sizes[0],\n",
    "                                  num_filters=num_filters[0],\n",
    "                                  use_pooling=True,\n",
    "                                  max_pool_size=max_pools[0])\n",
    "    \n",
    "    network = tf.nn.dropout(network, keep_prob=keep_prob)\n",
    "    network, _ = cn.new_conv_layer(input=network,\n",
    "                                  num_input_channels=num_filters[0],\n",
    "                                  filter_size=filter_sizes[1],\n",
    "                                  num_filters=num_filters[1],\n",
    "                                  use_pooling=True,\n",
    "                                  max_pool_size=max_pools[1])\n",
    "    network = tf.nn.dropout(network, keep_prob=keep_prob)\n",
    "    network, num_fc_features = cn.flatten_layer(network) # 256\n",
    "    \n",
    "    # Flatten and build the fully-connected layers.\n",
    "    network, _ = cn.new_fc_layer(input=network,\n",
    "                             num_inputs=num_fc_features,\n",
    "                             num_outputs=fc_1_size,\n",
    "                             use_relu=False)\n",
    "    network, _ = cn.new_fc_layer(input=network,          \n",
    "                             num_inputs=fc_1_size,\n",
    "                             num_outputs=num_classes,\n",
    "                             use_relu=True)\n",
    "\n",
    "    y_pred = tf.nn.softmax(network)                    \n",
    "    y_pred_cls = tf.argmax(network, dimension=1)\n",
    "    \n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=network, labels=y_true_)\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    return y_pred, y_pred_cls, cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split operations (optionally) across GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "    in_splits = {}\n",
    "    for k, v in kwargs.items():\n",
    "        in_splits[k] = tf.split(v, num_gpus)\n",
    "\n",
    "    y_pred_split, y_pred_cls_split, cost_split = [], [], []\n",
    "    for i in range(num_gpus):\n",
    "        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n",
    "            with tf.variable_scope(tf.get_variable_scope(), reuse=i > 0):\n",
    "                y_pred_, y_pred_cls_, cost_ = fn(**{k : v[i] for k, v in in_splits.items()})\n",
    "                y_pred_split.append(y_pred_)\n",
    "                y_pred_cls_split.append(y_pred_cls_)\n",
    "                cost_split.append(cost_)\n",
    "\n",
    "    return tf.concat(y_pred_split, axis=0), tf.concat(y_pred_cls_split, axis=0), tf.stack(cost_split, axis=0)\n",
    "\n",
    "if num_gpus > 0:\n",
    "    total_batches = num_gpus\n",
    "else:\n",
    "    total_batches = 1\n",
    "\n",
    "# train_batch_size = train_batch_size * total_batches\n",
    "# test_batch_size = test_batch_size * total_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define cost and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D_6:0\", shape=(?, 16, 16, 16), dtype=float32)\n",
      "Tensor(\"Relu_9:0\", shape=(?, 16, 16, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_6:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      "Tensor(\"Conv2D_7:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      "Tensor(\"Relu_10:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_7:0\", shape=(?, 4, 4, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "if num_gpus > 0:\n",
    "    # Remember that this adds significant latency for CPU<->GPU copying of shared variables.\n",
    "    # 2 GPU's is enough to get a good balance between speedup and minimal latency (12 GB on k80 nodes)\n",
    "    y_pred, y_pred_cls, cost = make_parallel(model, num_gpus, x_image_=x_image, y_true_=y_true)\n",
    "else:\n",
    "    # CPU-only version\n",
    "    y_pred, y_pred_cls, cost = model(x_image_=x_image, y_true_=y_true)\n",
    "    \n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if num_gpus > 0: # Log GPU/CPU placement to the terminal if using GPU's.\n",
    "    session = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "else:\n",
    "    session = tf.Session()\n",
    "    \n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver() # For when we want to save the model.\n",
    "\n",
    "# Global ounter for total number of iterations performed so far.\n",
    "total_iterations = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supporting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(iterations=False):\n",
    "    model_name = 'm-' + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\") + \"-\"\n",
    "    if iterations:\n",
    "        model_name += str(iterations)\n",
    "    else:\n",
    "        model_name += str(total_iterations)\n",
    "    \n",
    "    save_path = saver.save(session, checkpoints_dir + model_name)\n",
    "    print(\"Model saved: \" + model_name)\n",
    "\n",
    "def restore_model(model_name):\n",
    "    saver.restore(sess=session, save_path=checkpoints_dir + model_name)\n",
    "    \n",
    "def optimize(dataset_train, num_iterations, dropout_keep_prob=0.9, print_opt_acc=False, silent=False):\n",
    "    global total_iterations\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(total_iterations,\n",
    "                   total_iterations + num_iterations):\n",
    "        x_batch, y_true_batch = dataset_train.next_batch(train_batch_size)\n",
    "        x_batch = x_batch.reshape(len(x_batch), img_size_flat)\n",
    "        feed_dict_train = {x: x_batch, y_true: y_true_batch, keep_prob: dropout_keep_prob}\n",
    "\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "\n",
    "        # Print status every few iterations (a big few).\n",
    "        if i % 200 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            if print_opt_acc:\n",
    "                msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "                print(msg.format(i + 1, acc))\n",
    "                \n",
    "        # Check if we should save the model more frequently than we print the status.\n",
    "        #if i % 1000 == 0:\n",
    "        #    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "        #    if acc > 0.65:\n",
    "        #        save_model()\n",
    "\n",
    "    total_iterations += num_iterations\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    if not silent:\n",
    "        print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "    \n",
    "def print_test_accuracy(dataset_test, show_confusion_matrix=True, quieter=False, silent=False):\n",
    "    num_test = len(dataset_test.images)\n",
    "    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n",
    "    i = 0\n",
    "\n",
    "    while i < num_test:\n",
    "        j = min(i + test_batch_size, num_test)\n",
    "        curr_batch_size = j - i\n",
    "        \n",
    "        # Get the images and targets from the test-set between index i and j.\n",
    "        images = dataset_test.images[i:j, :].reshape(curr_batch_size, img_size_flat)\n",
    "        labels = dataset_test.labels[i:j, :]\n",
    "        feed_dict = {x: images, y_true: labels, keep_prob: 1.0}\n",
    "\n",
    "        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n",
    "        i = j\n",
    "\n",
    "    cls_true = dataset_test.cls\n",
    "    \n",
    "    # Create a boolean array whether each image is correctly classified.\n",
    "    correct = (cls_true == cls_pred)\n",
    "\n",
    "    correct_sum = correct.sum() #sum(1 for a, b in zip(cls_true, cls_pred) if a and b)\n",
    "    acc = float(correct_sum) / num_test\n",
    "\n",
    "    msg = \"Accuracy on validation set: {0:.1%} ({1} / {2})\"\n",
    "    if not quieter:\n",
    "        print(msg.format(acc, correct_sum, num_test))\n",
    "    else:\n",
    "        if not silent:\n",
    "            print(\"{0:.1%}\".format(acc))\n",
    "    if show_confusion_matrix:\n",
    "        cn.plot_confusion_matrix(cls_true, cls_pred=cls_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
